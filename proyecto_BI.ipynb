{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZT1ATFVVWNO"
      },
      "source": [
        "# Proyecto de Clasificación de Noticias Falsas en Español\n",
        "\n",
        "Este notebook contiene la implementación de un modelo de Machine Learning para clasificar noticias como verdaderas o falsas. Se utilizan técnicas de procesamiento de lenguaje natural para analizar el contenido de los títulos y descripciones de las noticias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0dks5tBVWNP"
      },
      "source": [
        "*Estudiante 1:* Rodrigo Paz Londoño.\n",
        "*Estudiante 2:*Juan José Murillo Aristizabal.\n",
        "*Estudiante 3:* Manolo Hernandez Rojas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDbQ1Wr3VWNP"
      },
      "source": [
        "## Instalación de Dependencias\n",
        "\n",
        "Se instalan las bibliotecas necesarias para el procesamiento de texto y el modelado de Machine Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Nq9tKhk9VWNP",
        "outputId": "0bf74999-e501-4eaf-9cb2-363f2705987e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting visualkeras\n",
            "  Downloading visualkeras-0.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from visualkeras) (11.1.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.11/dist-packages (from visualkeras) (1.26.4)\n",
            "Collecting aggdraw>=1.3.11 (from visualkeras)\n",
            "  Downloading aggdraw-1.3.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (655 bytes)\n",
            "Downloading visualkeras-0.1.4-py3-none-any.whl (17 kB)\n",
            "Downloading aggdraw-1.3.19-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (997 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.4/997.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: aggdraw, visualkeras\n",
            "Successfully installed aggdraw-1.3.19 visualkeras-0.1.4\n",
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn\n",
        "!pip install spacy\n",
        "!pip install scikit-learn\n",
        "!pip install nltk\n",
        "!python -m spacy download es_core_news_sm\n",
        "!python -m spacy download es_core_news_sm\n",
        "!pip install joblib\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install scikeras\n",
        "!pip install matplotlib\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install gensim\n",
        "!pip install wordcloud\n",
        "!pip install sklearn\n",
        "!pip install visualkeras\n",
        "!pip install --upgrade scikeras scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QMHHV2UVWNP"
      },
      "source": [
        "## Importación de Bibliotecas\n",
        "\n",
        "Se importan las bibliotecas necesarias para el análisis de datos, procesamiento de texto y modelado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "juYkfCVuVWNQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re, unicodedata\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import  confusion_matrix\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from joblib import dump, load\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9r-i6QHVWNQ"
      },
      "source": [
        "## Carga de Datos\n",
        "\n",
        "Se carga el dataset con noticias en español desde un archivo CSV. Aseguramos que la columna de fecha esté en el formato adecuado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xnOlZ7naiGw",
        "outputId": "1a2baf34-0655-464c-ddbd-b9bf925268c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ID  Label                                             Titulo  \\\n",
            "0  ID      1  'The Guardian' va con Sánchez: 'Europa necesit...   \n",
            "1  ID      0  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
            "2  ID      1  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
            "3  ID      1  Iglesias alienta a Yolanda Díaz, ERC y EH Bild...   \n",
            "4  ID      0  Puigdemont: 'No sería ninguna tragedia una rep...   \n",
            "\n",
            "                                         Descripcion      Fecha  \n",
            "0  El diario británico publicó este pasado jueves... 2023-06-02  \n",
            "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ... 2023-10-01  \n",
            "2  El valencianismo convoca en Castelló su fiesta... 2022-04-25  \n",
            "3  En política, igual que hay que negociar con lo... 2022-01-03  \n",
            "4  En una entrevista en El Punt Avui, el líder de... 2018-03-09  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Uso de la libreria pandas para la lectura de archivos\n",
        "data = pd.read_csv('fake_news_spanish.csv', sep=';', encoding='utf-8')\n",
        "# Asignación a una nueva variable de los datos leidos\n",
        "data_t = data\n",
        "\n",
        "\n",
        "data_t[\"Fecha\"] = pd.to_datetime(data_t[\"Fecha\"], format=\"%d/%m/%Y\")\n",
        "\n",
        "print(data_t.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUfiMV_PVWNQ"
      },
      "source": [
        "## Exploración de Datos\n",
        "\n",
        "Se examina la estructura de los datos para entender su composición y detectar valores nulos o inconsistencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFB8LXiXcWQH",
        "outputId": "1ee3e4bd-b1fd-4511-edf3-98a2887f4879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 57063 entries, 0 to 57062\n",
            "Data columns (total 5 columns):\n",
            " #   Column       Non-Null Count  Dtype         \n",
            "---  ------       --------------  -----         \n",
            " 0   ID           57063 non-null  object        \n",
            " 1   Label        57063 non-null  int64         \n",
            " 2   Titulo       57047 non-null  object        \n",
            " 3   Descripcion  57063 non-null  object        \n",
            " 4   Fecha        57063 non-null  datetime64[ns]\n",
            "dtypes: datetime64[ns](1), int64(1), object(3)\n",
            "memory usage: 2.2+ MB\n"
          ]
        }
      ],
      "source": [
        "data_t.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raBG9EPcVWNQ"
      },
      "source": [
        "## Análisis de Correlación entre Fecha y Etiqueta\n",
        "\n",
        "Convertimos la fecha en un valor numérico y analizamos su correlación con la etiqueta de veracidad de la noticia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BGxUxPpTVWNQ",
        "outputId": "8b7e8ddc-c61c-4813-dc35-65c7e58d84d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# Convertimos la fecha a ordinal\n",
        "data_t['Fecha_ordinal'] = data_t['Fecha'].apply(lambda x: x.toordinal())\n",
        "\n",
        "# Calculamos la matriz de correlación con Label\n",
        "corr_matrix = data_t[['Fecha_ordinal', 'Label']].corr()\n",
        "\n",
        "# Graficar la matriz de correlación\n",
        "plt.figure(figsize=(4, 3))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='Blues')\n",
        "plt.title('Correlación entre Fecha y Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTabGzL7VlBO"
      },
      "source": [
        "Se puede decir que la fecha no tiene una correlación relevante con la variable de interés \"Label\", por ello, se decide que no se incluirá en el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T79MxavjWXBg"
      },
      "source": [
        "##Análisis de Longitud de Textos en Títulos y Descripciones\n",
        "\n",
        "En esta sección, se realiza un análisis de la estructura de los textos en las noticias. Se calculan métricas clave sobre los títulos y descripciones para comprender mejor la distribución de palabras y su impacto en el modelo.\n",
        "\n",
        "*   Relleno de valores nulos: Se reemplazan los valores faltantes en la columna 'Titulo' con una cadena vacía para evitar errores en el procesamiento.\n",
        "*   Conteo de caracteres: Se calcula la longitud total de cada descripción y título.\n",
        "*  Palabra más larga y más corta: Se extrae la longitud de la palabra más larga y más corta en cada descripción y título, lo que permite analizar la complejidad del vocabulario.\n",
        "\n",
        "Este análisis proporciona información útil para entender la variabilidad en la longitud del texto, lo que puede influir en el rendimiento del modelo de clasificació\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p9kXCUYNc47p"
      },
      "outputs": [],
      "source": [
        "textos = data_t.copy()\n",
        "textos['Titulo'] = textos['Titulo'].fillna(\"\")\n",
        "textos['Conteo Descripcion'] = [len(x) for x in textos['Descripcion']]\n",
        "textos['Conteo Titulo'] = [len(x) for x in textos['Titulo']]\n",
        "textos['Max Descripcion'] = [[max([len(x) for x in i.split(' ')])][0] for i in textos['Descripcion']]\n",
        "textos['Max Titulo'] = [[max([len(x) for x in i.split(' ')])][0] for i in textos['Titulo']]\n",
        "textos['Min Descripcion'] = [[min([len(x) for x in i.split(' ')])][0] for i in textos['Descripcion']]\n",
        "textos['Min Titulo'] = [[min([len(x) for x in i.split(' ')])][0] for i in textos['Titulo']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCE-qMolVWNQ"
      },
      "source": [
        "## Preprocesamiento de Texto\n",
        "\n",
        "Se aplica procesamiento de texto utilizando spaCy para eliminar stopwords, realizar lematización y normalizar el texto, de igual forma se crean funciones con el fin de eliminar caracteres no ASCII, convertir todo a minuscula, remover puntuación, elminar palabras vacías."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXYxtu4LglbQ",
        "outputId": "7afa8c34-0fae-48f8-a087-757939ae7f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"\n",
        "    Elimina caracteres no ASCII de una lista de palabras tokenizadas.\n",
        "    Nota: Esto removerá acentos y caracteres especiales del español.\n",
        "    \"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word is not None:\n",
        "            new_word = unicodedata.normalize('NFKD', word)\\\n",
        "                      .encode('ascii', 'ignore')\\\n",
        "                      .decode('utf-8', 'ignore')\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"\n",
        "    Convierte todos los caracteres a minúsculas a partir de una lista de palabras tokenizadas.\n",
        "    \"\"\"\n",
        "    return [word.lower() for word in words if word is not None]\n",
        "\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"\n",
        "    Elimina la puntuación de una lista de palabras tokenizadas.\n",
        "    \"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word is not None:\n",
        "            # Se eliminan todos los caracteres que no sean letras, dígitos o espacios.\n",
        "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "            if new_word != '':\n",
        "                new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"\n",
        "    Elimina las palabras vacías (stopwords) de una lista de palabras tokenizadas usando\n",
        "    la lista de stopwords en español de NLTK.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('spanish'))\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "def preprocessing(words):\n",
        "    \"\"\"\n",
        "    Aplica una serie de transformaciones a una lista de palabras tokenizadas:\n",
        "      - Convertir a minúsculas.\n",
        "      - Eliminar puntuación.\n",
        "      - Eliminar caracteres no ASCII (opcional, cuidado con acentos).\n",
        "      - Eliminar palabras vacías.\n",
        "    \"\"\"\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = remove_non_ascii(words)\n",
        "    words = remove_stopwords(words)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "SdGaG-zThGGr",
        "outputId": "6e3f1abe-bf98-43da-fcd2-bf4eee4904c4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-651f0704ea0f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Aplicamos la función a la columna 'Descripcion'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdata_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Descripcion_processed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Descripcion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdata_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Titulo_processed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Titulo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-651f0704ea0f>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Procesamos cada token:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# - Convertimos a minúscula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1047\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/pipeline/trainable_pipe.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/pipeline/tok2vec.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mtokvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokvecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \"\"\"\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/with_array.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSeqT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_list_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/with_array.py\u001b[0m in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUMPY_OPS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray1i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mXf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mYf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dXf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdYs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mListXd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mListXd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/residual.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0md_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/layernorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mXhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_rescale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_begin_update_scale_shift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/thinc/layers/layernorm.py\u001b[0m in \u001b[0;36m_get_moments\u001b[0;34m(ops, X)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFloats2d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFloats2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloats2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloats2d\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# TODO: Do mean methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mmu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFloats2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mvar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFloats2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-08\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFloats2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Cargar el modelo de español\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Suponiendo que data_t es tu DataFrame y tiene la columna 'Descripcion'\n",
        "# Definimos una función de preprocesamiento con spaCy\n",
        "def preprocess_text(text):\n",
        "    # Verificar que el texto no sea NaN\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    doc = nlp(text)\n",
        "    # Procesamos cada token:\n",
        "    # - Convertimos a minúscula\n",
        "    # - Extraemos el lema\n",
        "    # - Eliminamos tokens de puntuación y stopwords\n",
        "    tokens = [\n",
        "        token.lemma_.lower()\n",
        "        for token in doc\n",
        "        if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-'\n",
        "    ]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Aplicamos la función a la columna 'Descripcion'\n",
        "data_t['Descripcion_processed'] = data_t['Descripcion'].apply(preprocess_text)\n",
        "data_t['Titulo_processed'] = data_t['Titulo'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# Mostramos algunos resultados\n",
        "print(data_t[['Descripcion', 'Descripcion_processed', 'Titulo','Titulo_processed']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMMc7GmTk01I"
      },
      "outputs": [],
      "source": [
        "# Función para tokenizar el texto procesado (dividir en tokens)\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "# Aplicamos la tokenización sobre el texto preprocesado\n",
        "data_t['Tokens_Descripcion'] = data_t['Descripcion_processed'].apply(tokenize_text)\n",
        "data_t['Tokens_Titulo'] = data_t['Titulo_processed'].apply(tokenize_text)\n",
        "\n",
        "\n",
        "# Mostramos algunos resultados\n",
        "print(data_t[['Descripcion','Tokens_Descripcion', 'Titulo','Tokens_Titulo']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnhX5xS0W3VE"
      },
      "source": [
        "Elimina los valores nulos de los tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm4vZQq9k-f4"
      },
      "outputs": [],
      "source": [
        "data_t['Tokens_Descripcion'].dropna()\n",
        "data_t['Tokens_Titulo'].dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV2ShJDbXcvO"
      },
      "source": [
        "Revisar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wu-ioFKlGgK"
      },
      "outputs": [],
      "source": [
        "data_t['Tokens_Descripcion'].info()\n",
        "data_t['Tokens_Titulo'].info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZC_7zPAXhKd"
      },
      "source": [
        "Se aplica el procesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6chK2R24lLt_"
      },
      "outputs": [],
      "source": [
        "data_t['Words_Descripcion']=data_t['Tokens_Descripcion'].apply(preprocessing)\n",
        "data_t['Words_Titulo']=data_t['Tokens_Titulo'].apply(preprocessing)\n",
        "\n",
        "data_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcCewCVHVWNR"
      },
      "source": [
        "# Entrenamiento de cada uno de los Modelos\n",
        "\n",
        "\n",
        "\n",
        "*   Árbol de Decisión\n",
        "*   Regresión Logística\n",
        "*   Red Neuronal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NggOgdGpiRe2"
      },
      "source": [
        "## Entrenamiento del árbol de decisión\n",
        "\n",
        "Se crea un pipeline para procesar el texto y entrenar un modelo de árbol de decisión para la clasificación de noticias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9-HuGYtiiif"
      },
      "outputs": [],
      "source": [
        "# Convertir listas en cadenas de texto para que TfidfVectorizer pueda procesarlas\n",
        "data_t[\"Words_Titulo\"] = data_t[\"Words_Titulo\"].apply(lambda x: \" \".join(x))\n",
        "data_t[\"Words_Descripcion\"] = data_t[\"Words_Descripcion\"].apply(lambda x: \" \".join(x))\n",
        "print(data_t[['Words_Titulo', 'Words_Descripcion']].head())\n",
        "# Creamos un nuevo DataFrame con las columnas transformadas\n",
        "df_text = pd.DataFrame({\n",
        "    \"Words_Titulo\": data_t[\"Words_Titulo\"],\n",
        "    \"Words_Descripcion\": data_t[\"Words_Descripcion\"],\n",
        "    \"Label\": data_t[\"Label\"]\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# Separar variables predictoras y variable objetivo\n",
        "X = data_t[['Words_Titulo', 'Words_Descripcion']]\n",
        "y = data_t['Label']\n",
        "\n",
        "# Dividir en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69)\n",
        "\n",
        "# Pipeline de preprocesamiento que incluye las características de texto\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('title_tfidf', TfidfVectorizer(), 'Words_Titulo'),\n",
        "    ('desc_tfidf', TfidfVectorizer(), 'Words_Descripcion')\n",
        "])\n",
        "\n",
        "# Crear el pipeline con un Árbol de Decisión\n",
        "pipeline = Pipeline([\n",
        "    ('features', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier(random_state=69))\n",
        "])\n",
        "\n",
        "# Entrenar el modelo\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrEaijSjkSdd"
      },
      "source": [
        "#### Evaluación del Modelo\n",
        "\n",
        "Se evalúa el desempeño del modelo en el conjunto de prueba utilizando métricas como precisión, recall y matriz de confusión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYi_3QFJkZSI"
      },
      "outputs": [],
      "source": [
        "# Realizar las predicciones en el conjunto de prueba\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "# En la matriz, se asume la siguiente estructura:\n",
        "# [[TN, FP],\n",
        "#  [FN, TP]]\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "# Calcular las métricas\n",
        "# Exactitud (Accuracy): (VP + VN) / Total\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "# Error: (FP + FN) / Total\n",
        "error_rate = (FP + FN) / (TP + TN + FP + FN)\n",
        "\n",
        "# Precisión: VP / (VP + FP)\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "\n",
        "# Sensibilidad o Recall: VP / (VP + FN)\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "# Especificidad: VN / (VN + FP)\n",
        "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "\n",
        "# F1-Score: 2 * (precision * recall) / (precision + recall)\n",
        "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Imprimir las métricas\n",
        "print(\"Métricas del modelo:\")\n",
        "print(f\"Exactitud (Accuracy): {accuracy:.2f}\")\n",
        "print(f\"Error: {error_rate:.2f}\")\n",
        "print(f\"Precisión (Precision): {precision:.2f}\")\n",
        "print(f\"Sensibilidad (Recall): {recall:.2f}\")\n",
        "print(f\"Especificidad: {specificity:.2f}\")\n",
        "print(f\"F1-Score: {f1_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC89JK-Yu2tQ"
      },
      "outputs": [],
      "source": [
        "# Obtener probabilidades de predicción\n",
        "y_probs = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calcular la curva ROC\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Graficar la curva AUC-ROC\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Línea diagonal\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title('Curva ROC - Árbol de Decisión')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cDxIxpVu2tQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "importancia = pipeline.named_steps['classifier'].feature_importances_\n",
        "\n",
        "# Obtener nombres de las características desde TF-IDF (título y descripción)\n",
        "vectorizer_title = pipeline.named_steps['features'].named_transformers_['title_tfidf']\n",
        "vectorizer_desc = pipeline.named_steps['features'].named_transformers_['desc_tfidf']\n",
        "\n",
        "# Extraer nombres de las palabras de cada vectorizador\n",
        "feature_names_title = vectorizer_title.get_feature_names_out()\n",
        "feature_names_desc = vectorizer_desc.get_feature_names_out()\n",
        "\n",
        "# Combinar los nombres de las características en un solo array\n",
        "feature_names = np.concatenate([feature_names_title, feature_names_desc])\n",
        "\n",
        "# Verificar que la cantidad de nombres y coeficientes coincida\n",
        "assert len(feature_names) == len(importancia), f\"Dimensiones no coinciden: {len(feature_names)} != {len(importancia)}\"\n",
        "\n",
        "# Crear un DataFrame con la importancia de cada palabra\n",
        "importancia_atributo = pd.DataFrame({\"Atributo\": feature_names, \"Importancia\": importancia})\n",
        "importancia_atributo = importancia_atributo.sort_values(by='Importancia', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Mostrar las 15 palabras más importantes\n",
        "print(\"🔹 Palabras más influyentes en la predicción:\")\n",
        "print(importancia_atributo.head(50))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importancia_atributo[\"Atributo\"][:50], importancia_atributo[\"Importancia\"][:50], color='blue')\n",
        "plt.xlabel(\"Importancia\")\n",
        "plt.ylabel(\"Palabra\")\n",
        "plt.title(\"Palabras más influyentes en la clasificación - Árbol de Decisión\")\n",
        "plt.gca().invert_yaxis()  # Invertir el eje para que la palabra más influyente aparezca arriba\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWgJ_uwOu2tR"
      },
      "outputs": [],
      "source": [
        "filename = \"model_decision_tree.joblib\"\n",
        "dump(pipeline, filename)\n",
        "pipeline_loaded = load(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3AgR65gu2tR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cargar el modelo previamente guardado\n",
        "filename = \"model_decision_tree.joblib\"\n",
        "pipeline_loaded = load(filename)\n",
        "\n",
        "# Cargar los datos de prueba\n",
        "test_data = pd.read_csv('fake_news_test.csv', sep=';', encoding='utf-8')\n",
        "\n",
        "# Cargar el modelo de español\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Suponiendo que data_t es tu DataFrame y tiene la columna 'Descripcion'\n",
        "# Definimos una función de preprocesamiento con spaCy\n",
        "def preprocess_text(text):\n",
        "    # Verificar que el texto no sea NaN\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    doc = nlp(text)\n",
        "    # Procesamos cada token:\n",
        "    # - Convertimos a minúscula\n",
        "    # - Extraemos el lema\n",
        "    # - Eliminamos tokens de puntuación y stopwords\n",
        "    tokens = [\n",
        "        token.lemma_.lower()\n",
        "        for token in doc\n",
        "        if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-'\n",
        "    ]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Aplicamos la función a la columna 'Descripcion'\n",
        "test_data['Descripcion_processed'] = test_data['Descripcion'].apply(preprocess_text)\n",
        "test_data['Titulo_processed'] = test_data['Titulo'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# Mostramos algunos resultados\n",
        "print(test_data[['Descripcion', 'Descripcion_processed', 'Titulo','Titulo_processed']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NW8CFusu2tR"
      },
      "outputs": [],
      "source": [
        "# Función para tokenizar el texto procesado (dividir en tokens)\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "# Aplicamos la tokenización sobre el texto preprocesado\n",
        "test_data['Tokens_Descripcion'] = test_data['Descripcion_processed'].apply(tokenize_text)\n",
        "test_data['Tokens_Titulo'] = test_data['Titulo_processed'].apply(tokenize_text)\n",
        "\n",
        "\n",
        "# Mostramos algunos resultados\n",
        "print(test_data[['Descripcion','Tokens_Descripcion', 'Titulo','Tokens_Titulo']].head())\n",
        "\n",
        "test_data['Tokens_Descripcion'].dropna()\n",
        "test_data['Tokens_Titulo'].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0StffKgu2tR"
      },
      "outputs": [],
      "source": [
        "test_data['Words_Descripcion']=test_data['Tokens_Descripcion'].apply(preprocessing)\n",
        "test_data['Words_Titulo']=test_data['Tokens_Titulo'].apply(preprocessing)\n",
        "\n",
        "test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iXvIstGu2tR"
      },
      "outputs": [],
      "source": [
        "# Verificar que las columnas correctas existen en los datos de prueba\n",
        "if 'Words_Titulo' in test_data.columns and 'Words_Descripcion' in test_data.columns:\n",
        "    # Convertir listas en cadenas de texto si es necesario\n",
        "    test_data[\"Words_Titulo\"] = test_data[\"Words_Titulo\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "    test_data[\"Words_Descripcion\"] = test_data[\"Words_Descripcion\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "\n",
        "    # Realizar predicciones\n",
        "    predictions = pipeline_loaded.predict(test_data[['Words_Titulo', 'Words_Descripcion']])\n",
        "\n",
        "    # Guardar los resultados en un nuevo CSV\n",
        "    test_data[\"Label\"] = predictions\n",
        "\n",
        "    #Borrar las columnas de texto procesado\n",
        "    test_data.drop(columns=['Descripcion_processed', 'Titulo_processed', 'Tokens_Descripcion', 'Tokens_Titulo','Words_Titulo','Words_Descripcion'], inplace=True)\n",
        "    test_data.to_csv(\"fake_news_predictions_decision_tree.csv\", index=False)\n",
        "\n",
        "    print(\"Predicciones guardadas en 'fake_news_predictions_decision_tree.csv'\")\n",
        "else:\n",
        "    print(\"Error: Las columnas 'Words_Titulo' y 'Words_Descripcion' no están en el archivo de prueba.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykBnHpkWY4Q7"
      },
      "source": [
        "\n",
        "\n",
        "## Entrenamiento de la regresión Logística\n",
        "\n",
        "Se crea un pipeline para procesar el texto y entrenar un modelo de regresión logística para la clasificación de noticias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsMBzZcI7tis"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Separar variables predictoras y variable objetivo\n",
        "X = data_t[['Words_Titulo', 'Words_Descripcion']]\n",
        "y = data_t['Label']\n",
        "\n",
        "# Dividir en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69)\n",
        "\n",
        "# Pipeline de preprocesamiento que incluye las características de texto\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('title_tfidf', TfidfVectorizer(), 'Words_Titulo'),\n",
        "    ('desc_tfidf', TfidfVectorizer(), 'Words_Descripcion')\n",
        "])\n",
        "\n",
        "# Pipeline completo con regresión logística\n",
        "pipeline = Pipeline([\n",
        "    ('features', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Entrenar el modelo\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQQSeZnGVWNR"
      },
      "source": [
        "#### Evaluación del Modelo\n",
        "\n",
        "Se evalúa el desempeño del modelo en el conjunto de prueba utilizando métricas como precisión, recall y matriz de confusión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45oOUYmIVWNR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Realizar las predicciones en el conjunto de prueba\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "# En la matriz, se asume la siguiente estructura:\n",
        "# [[TN, FP],\n",
        "#  [FN, TP]]\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "# Calcular las métricas\n",
        "# Exactitud (Accuracy): (VP + VN) / Total\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "# Error: (FP + FN) / Total\n",
        "error_rate = (FP + FN) / (TP + TN + FP + FN)\n",
        "\n",
        "# Precisión: VP / (VP + FP)\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "\n",
        "# Sensibilidad o Recall: VP / (VP + FN)\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "# Especificidad: VN / (VN + FP)\n",
        "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "\n",
        "# F1-Score: 2 * (precision * recall) / (precision + recall)\n",
        "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Imprimir las métricas\n",
        "print(\"Métricas del modelo:\")\n",
        "print(f\"Exactitud (Accuracy): {accuracy:.2f}\")\n",
        "print(f\"Error: {error_rate:.2f}\")\n",
        "print(f\"Precisión (Precision): {precision:.2f}\")\n",
        "print(f\"Sensibilidad (Recall): {recall:.2f}\")\n",
        "print(f\"Especificidad: {specificity:.2f}\")\n",
        "print(f\"F1-Score: {f1_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvB-ISq9D77n"
      },
      "outputs": [],
      "source": [
        "# Obtener probabilidades de predicción\n",
        "y_probs = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calcular la curva ROC\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Graficar la curva AUC-ROC\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Línea diagonal\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title('Curva ROC - Regresión Logística')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgSUxRPpaFqf"
      },
      "outputs": [],
      "source": [
        "# Obtener las transformaciones de ColumnTransformer\n",
        "vectorizer_titulo = pipeline.named_steps['features'].named_transformers_['title_tfidf']\n",
        "vectorizer_desc = pipeline.named_steps['features'].named_transformers_['desc_tfidf']\n",
        "\n",
        "# Obtener los nombres de las palabras procesadas en cada vectorizador\n",
        "feature_names_titulo = vectorizer_titulo.get_feature_names_out()\n",
        "feature_names_desc = vectorizer_desc.get_feature_names_out()\n",
        "\n",
        "# Combinar todas las palabras\n",
        "feature_names = np.concatenate([feature_names_titulo, feature_names_desc])\n",
        "\n",
        "# Obtener los coeficientes del modelo de regresión logística\n",
        "coefs = pipeline.named_steps['classifier'].coef_[0]\n",
        "\n",
        "# Verificar que las longitudes coincidan\n",
        "assert len(feature_names) == len(coefs), f\"Dimensiones no coinciden: {len(feature_names)} != {len(coefs)}\"\n",
        "\n",
        "# Crear un DataFrame con palabras y sus coeficientes\n",
        "word_importance = pd.DataFrame({'Palabra': feature_names, 'Coeficiente': coefs})\n",
        "\n",
        "# Ordenar palabras más asociadas con noticias falsas (coeficiente más negativo)\n",
        "top_false_words = word_importance.sort_values(by='Coeficiente', ascending=True).head(50)\n",
        "\n",
        "# Graficar las palabras más asociadas con noticias falsas\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_false_words['Palabra'], top_false_words['Coeficiente'], color='red')\n",
        "plt.xlabel('Coeficiente')\n",
        "plt.ylabel('Palabra')\n",
        "plt.title('Palabras más asociadas con noticias falsas')\n",
        "plt.gca().invert_yaxis()  # Invertir eje para mostrar la palabra más relevante arriba\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KIWsnWpVWNR"
      },
      "outputs": [],
      "source": [
        "filename = \"model_logistic_regression.joblib\"\n",
        "dump(pipeline, filename)\n",
        "pipeline_loaded = load(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRO5h2gcQq3u"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cargar el modelo previamente guardado\n",
        "filename = \"model_logistic_regression.joblib\"\n",
        "pipeline_loaded = load(filename)\n",
        "\n",
        "# Cargar los datos de prueba\n",
        "test_data = pd.read_csv('fake_news_test.csv', sep=';', encoding='utf-8')\n",
        "\n",
        "# Cargar el modelo de español\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Suponiendo que data_t es tu DataFrame y tiene la columna 'Descripcion'\n",
        "# Definimos una función de preprocesamiento con spaCy\n",
        "def preprocess_text(text):\n",
        "    # Verificar que el texto no sea NaN\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    doc = nlp(text)\n",
        "    # Procesamos cada token:\n",
        "    # - Convertimos a minúscula\n",
        "    # - Extraemos el lema\n",
        "    # - Eliminamos tokens de puntuación y stopwords\n",
        "    tokens = [\n",
        "        token.lemma_.lower()\n",
        "        for token in doc\n",
        "        if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-'\n",
        "    ]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Aplicamos la función a la columna 'Descripcion'\n",
        "test_data['Descripcion_processed'] = test_data['Descripcion'].apply(preprocess_text)\n",
        "test_data['Titulo_processed'] = test_data['Titulo'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# Mostramos algunos resultados\n",
        "print(test_data[['Descripcion', 'Descripcion_processed', 'Titulo','Titulo_processed']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss_FWydRrwZk"
      },
      "outputs": [],
      "source": [
        "# Función para tokenizar el texto procesado (dividir en tokens)\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "# Aplicamos la tokenización sobre el texto preprocesado\n",
        "test_data['Tokens_Descripcion'] = test_data['Descripcion_processed'].apply(tokenize_text)\n",
        "test_data['Tokens_Titulo'] = test_data['Titulo_processed'].apply(tokenize_text)\n",
        "\n",
        "\n",
        "# Mostramos algunos resultados\n",
        "print(test_data[['Descripcion','Tokens_Descripcion', 'Titulo','Tokens_Titulo']].head())\n",
        "\n",
        "test_data['Tokens_Descripcion'].dropna()\n",
        "test_data['Tokens_Titulo'].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKTAPeb0r_i0"
      },
      "outputs": [],
      "source": [
        "test_data['Words_Descripcion']=test_data['Tokens_Descripcion'].apply(preprocessing)\n",
        "test_data['Words_Titulo']=test_data['Tokens_Titulo'].apply(preprocessing)\n",
        "\n",
        "test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftUFX_8HssJA"
      },
      "outputs": [],
      "source": [
        "# Verificar que las columnas correctas existen en los datos de prueba\n",
        "if 'Words_Titulo' in test_data.columns and 'Words_Descripcion' in test_data.columns:\n",
        "    # Convertir listas en cadenas de texto si es necesario\n",
        "    test_data[\"Words_Titulo\"] = test_data[\"Words_Titulo\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "    test_data[\"Words_Descripcion\"] = test_data[\"Words_Descripcion\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "\n",
        "    # Realizar predicciones\n",
        "    predictions = pipeline_loaded.predict(test_data[['Words_Titulo', 'Words_Descripcion']])\n",
        "\n",
        "    # Guardar los resultados en un nuevo CSV\n",
        "    test_data[\"Label\"] = predictions\n",
        "\n",
        "    #Borrar las columnas de texto procesado\n",
        "    test_data.drop(columns=['Descripcion_processed', 'Titulo_processed', 'Tokens_Descripcion', 'Tokens_Titulo','Words_Titulo','Words_Descripcion'], inplace=True)\n",
        "    test_data.to_csv(\"fake_news_predictions_logistic_regression.csv\", index=False)\n",
        "\n",
        "    print(\"Predicciones guardadas en 'fake_news_predictions_logistic_regression.csv'\")\n",
        "else:\n",
        "    print(\"Error: Las columnas 'Words_Titulo' y 'Words_Descripcion' no están en el archivo de prueba.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA6odWnFxBMo"
      },
      "source": [
        "\n",
        "\n",
        "## Entrenamiento de la red neuronal\n",
        "\n",
        "Se crea un pipeline para procesar el texto y entrenar un modelo de red neuronal para la clasificación de noticias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9ydGtJngfam"
      },
      "outputs": [],
      "source": [
        "X = df_text[['Words_Titulo', 'Words_Descripcion']]\n",
        "y = df_text['Label']\n",
        "dataset=df_text.to_numpy()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=69)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7lkrsBIgfam"
      },
      "outputs": [],
      "source": [
        "preprocessor = ColumnTransformer([\n",
        "    ('title_tfidf', TfidfVectorizer(), 'Words_Titulo'),\n",
        "    ('desc_tfidf', TfidfVectorizer(), 'Words_Descripcion')\n",
        "])\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', MLPClassifier(hidden_layer_sizes=(50,), random_state=42, max_iter=1500))\n",
        "])\n",
        "pipeline.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kxnmaCMgfan"
      },
      "source": [
        "#### Evaluación del Modelo\n",
        "\n",
        "Se evalúa el desempeño del modelo en el conjunto de prueba utilizando métricas como precisión, recall y matriz de confusión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InqYtU-Ogfan"
      },
      "outputs": [],
      "source": [
        "# Realizar las predicciones en el conjunto de prueba\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "# En la matriz, se asume la siguiente estructura:\n",
        "# [[TN, FP],\n",
        "#  [FN, TP]]\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "# Calcular las métricas\n",
        "# Exactitud (Accuracy): (VP + VN) / Total\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "# Error: (FP + FN) / Total\n",
        "error_rate = (FP + FN) / (TP + TN + FP + FN)\n",
        "\n",
        "# Precisión: VP / (VP + FP)\n",
        "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "\n",
        "# Sensibilidad o Recall: VP / (VP + FN)\n",
        "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "# Especificidad: VN / (VN + FP)\n",
        "specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "\n",
        "# F1-Score: 2 * (precision * recall) / (precision + recall)\n",
        "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Imprimir las métricas\n",
        "print(\"Métricas del modelo:\")\n",
        "print(f\"Exactitud (Accuracy): {accuracy:.2f}\")\n",
        "print(f\"Error: {error_rate:.2f}\")\n",
        "print(f\"Precisión (Precision): {precision:.2f}\")\n",
        "print(f\"Sensibilidad (Recall): {recall:.2f}\")\n",
        "print(f\"Especificidad: {specificity:.2f}\")\n",
        "print(f\"F1-Score: {f1_score:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TorJn13Fgfan"
      },
      "outputs": [],
      "source": [
        "# Obtener probabilidades de predicción\n",
        "y_probs = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calcular la curva ROC\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Graficar la curva AUC-ROC\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Línea diagonal\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title('Curva ROC - Red Neuronal')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNyM5Ye8gfan"
      },
      "outputs": [],
      "source": [
        "# 1. Filtrar el conjunto de entrenamiento para quedarnos con las noticias falsas.\n",
        "# Ajusta la condición según la codificación de tus etiquetas.\n",
        "false_mask = y_train == 0\n",
        "X_false = X_train[false_mask]\n",
        "\n",
        "# 2. Transformar el subconjunto de noticias falsas usando el preprocesador del pipeline\n",
        "X_false_transformed = pipeline.named_steps['preprocessor'].transform(X_false)\n",
        "\n",
        "# 3. Sumar los valores TF-IDF de cada palabra (cada columna de la matriz transformada)\n",
        "# La matriz es sparse, por lo que se suma a lo largo de los registros (axis=0)\n",
        "tfidf_sum = X_false_transformed.sum(axis=0)\n",
        "tfidf_sum_array = np.array(tfidf_sum).flatten()\n",
        "\n",
        "# 4. Extraer los nombres de las palabras de cada vectorizador del ColumnTransformer\n",
        "vectorizer_titulo = pipeline.named_steps['preprocessor'].named_transformers_['title_tfidf']\n",
        "vectorizer_desc = pipeline.named_steps['preprocessor'].named_transformers_['desc_tfidf']\n",
        "\n",
        "feature_names_titulo = vectorizer_titulo.get_feature_names_out()\n",
        "feature_names_desc = vectorizer_desc.get_feature_names_out()\n",
        "\n",
        "# 5. Combinar los nombres de las palabras en un solo arreglo\n",
        "feature_names = np.concatenate([feature_names_titulo, feature_names_desc])\n",
        "\n",
        "# 6. Verificar que el número de palabras coincida con la cantidad de columnas de la matriz TF-IDF\n",
        "assert len(feature_names) == len(tfidf_sum_array), f\"Dimensiones no coinciden: {len(feature_names)} != {len(tfidf_sum_array)}\"\n",
        "\n",
        "# 7. Crear un DataFrame que asocia cada palabra con la suma de su TF-IDF en las noticias falsas\n",
        "word_importance = pd.DataFrame({'Palabra': feature_names, 'TFIDF_Suma': tfidf_sum_array})\n",
        "\n",
        "# 8. Ordenar las palabras de mayor a menor según su suma TF-IDF y seleccionar las 50 principales\n",
        "top_false_words = word_importance.sort_values(by='TFIDF_Suma', ascending=False).head(50)\n",
        "\n",
        "# 9. Graficar las 50 palabras más representativas\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_false_words['Palabra'], top_false_words['TFIDF_Suma'], color='red')\n",
        "plt.xlabel('Cantidad de palabras')\n",
        "plt.ylabel('Palabra')\n",
        "plt.title('Palabras más asociadas con noticias falsas')\n",
        "plt.gca().invert_yaxis()  # Invertir eje para mostrar la palabra más relevante arriba\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIQbat-1gfan"
      },
      "outputs": [],
      "source": [
        "filename = \"model_neural_network.joblib\"\n",
        "dump(pipeline, filename)\n",
        "pipeline_loaded = load(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAaluCx_gfan"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo previamente guardado\n",
        "filename = \"model_neural_network.joblib\"\n",
        "pipeline_loaded = load(filename)\n",
        "\n",
        "# Cargar los datos de prueba\n",
        "test_data = pd.read_csv('fake_news_test.csv', sep=';', encoding='utf-8')\n",
        "\n",
        "# Cargar el modelo de español\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Suponiendo que data_t es tu DataFrame y tiene la columna 'Descripcion'\n",
        "# Definimos una función de preprocesamiento con spaCy\n",
        "def preprocess_text(text):\n",
        "    # Verificar que el texto no sea NaN\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    doc = nlp(text)\n",
        "    # Procesamos cada token:\n",
        "    # - Convertimos a minúscula\n",
        "    # - Extraemos el lema\n",
        "    # - Eliminamos tokens de puntuación y stopwords\n",
        "    tokens = [\n",
        "        token.lemma_.lower()\n",
        "        for token in doc\n",
        "        if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-'\n",
        "    ]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Aplicamos la función a la columna 'Descripcion'\n",
        "test_data['Descripcion_processed'] = test_data['Descripcion'].apply(preprocess_text)\n",
        "test_data['Titulo_processed'] = test_data['Titulo'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# Mostramos algunos resultados\n",
        "print(test_data[['Descripcion', 'Descripcion_processed', 'Titulo','Titulo_processed']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JqmG6hqgfan"
      },
      "outputs": [],
      "source": [
        "# Función para tokenizar el texto procesado (dividir en tokens)\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "# Aplicamos la tokenización sobre el texto preprocesado\n",
        "test_data['Tokens_Descripcion'] = test_data['Descripcion_processed'].apply(tokenize_text)\n",
        "test_data['Tokens_Titulo'] = test_data['Titulo_processed'].apply(tokenize_text)\n",
        "\n",
        "\n",
        "# Mostramos algunos resultados\n",
        "print(test_data[['Descripcion','Tokens_Descripcion', 'Titulo','Tokens_Titulo']].head())\n",
        "\n",
        "test_data['Tokens_Descripcion'].dropna()\n",
        "test_data['Tokens_Titulo'].dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzMCTGvggfan"
      },
      "outputs": [],
      "source": [
        "test_data['Words_Descripcion']=test_data['Tokens_Descripcion'].apply(preprocessing)\n",
        "test_data['Words_Titulo']=test_data['Tokens_Titulo'].apply(preprocessing)\n",
        "\n",
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "ruby"
        },
        "id": "MaGbyqEqgfan"
      },
      "outputs": [],
      "source": [
        "# Verificar que las columnas correctas existen en los datos de prueba\n",
        "if 'Words_Titulo' in test_data.columns and 'Words_Descripcion' in test_data.columns:\n",
        "    # Convertir listas en cadenas de texto si es necesario\n",
        "    test_data[\"Words_Titulo\"] = test_data[\"Words_Titulo\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "    test_data[\"Words_Descripcion\"] = test_data[\"Words_Descripcion\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else x)\n",
        "\n",
        "    # Realizar predicciones\n",
        "    predictions = pipeline_loaded.predict(test_data[['Words_Titulo', 'Words_Descripcion']])\n",
        "\n",
        "    # Guardar los resultados en un nuevo CSV\n",
        "    test_data[\"Label\"] = predictions\n",
        "\n",
        "    #Borrar las columnas de texto procesado\n",
        "    test_data.drop(columns=['Descripcion_processed', 'Titulo_processed', 'Tokens_Descripcion', 'Tokens_Titulo','Words_Titulo','Words_Descripcion'], inplace=True)\n",
        "    test_data.to_csv(\"fake_news_predictions_neural_network.csv\", index=False)\n",
        "\n",
        "    print(\"Predicciones guardadas en 'fake_news_predictions_neural_network.csv'\")\n",
        "else:\n",
        "    print(\"Error: Las columnas 'Words_Titulo' y 'Words_Descripcion' no están en el archivo de prueba.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}